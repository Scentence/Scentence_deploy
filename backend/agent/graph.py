# backend/agent/graph.py
import json
import asyncio
import random
import uuid
from typing import List, Dict, Any, Optional, Set

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI  # type: ignore[reportMissingImports]
from langchain_core.messages import (  # type: ignore[reportMissingImports]
    SystemMessage,
    AIMessage,
    HumanMessage,
)
from langgraph.graph import StateGraph, START, END  # type: ignore[reportMissingImports]
from langgraph.checkpoint.memory import MemorySaver  # type: ignore[reportMissingImports]

# [Import] ë¡œì»¬ ëª¨ë“ˆ
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision,
    ValidationResult,
    SearchStrategyPlan,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

# [Import] Expression Loader for dynamic dictionary injection
from .expression_loader import ExpressionLoader
from .brand_exclusion_parser import parse_brand_exclusions, should_clear_brand_fields

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
    smart_perfume_search,
)

from .prompts import (
    PRE_VALIDATOR_PROMPT,
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_RECOMMENDATION_PROMPT_SINGLE,
    WRITER_RECOMMENDATION_PROMPT_EXPERT_SINGLE,
)
from .database import save_recommendation_log, fetch_meta_data
from .denylist import has_forbidden_words, UserFriendlyStrategyLabels

from .followup_classifier import classify_followup
from .personalization import get_personalization_summary
from .use_case_utils import infer_use_case

# [ì •ë³´ ê²€ìƒ‰ ì „ìš© ì„œë¸Œ ê·¸ë˜í”„ ì„í¬íŠ¸]
from .graph_info import info_graph

load_dotenv()

import logging

logger = logging.getLogger(__name__)

# NOTE: Imported for monkeypatching in tests.
_MONKEYPATCH_TOOLS = (lookup_note_by_string_tool, lookup_note_by_vector_tool)

# ==========================================
# 0. Helper Functions (moved to utils.py)
# ==========================================
from .utils import (
    parse_recommended_count,
    normalize_recommended_count,
    extract_save_refs,
    sanitize_filters,
)

# ==========================================
# 1. ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4.1-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4.1", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)
# Non-streaming version for parallel_reco to prevent token interleaving
SUPER_SMART_LLM_NO_STREAM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=False)


# ==========================================
# 2. ìœ í‹¸ë¦¬í‹°
# ==========================================
def log_filters(h_filters: dict, s_filters: dict):
    pass


def generate_pre_notice(requested: int, is_explicit: bool) -> str:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì „ì— ì¶œë ¥ ê°€ëŠ¥í•œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    (ì¼€ì´ìŠ¤ 1: ê³¼ë‹¤ ìš”ì²­)

    Args:
        requested: ìš”ì²­ëœ ê°œìˆ˜
        is_explicit: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ê°œìˆ˜ë¥¼ ìš”ì²­í–ˆëŠ”ì§€

    Returns:
        ì•ˆë‚´ ë©”ì‹œì§€ (í•„ìš” ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    """
    MAX_COUNT = 5

    # ì¼€ì´ìŠ¤ 1: ê³¼ë‹¤ ìš”ì²­ (ëª…ì‹œì ì¼ ë•Œë§Œ)
    if is_explicit and requested > MAX_COUNT:
        return (
            f"\nğŸ’¡ ì•ˆë‚´: í•œ ë²ˆì— ìµœëŒ€ {MAX_COUNT}ê°œê¹Œì§€ë§Œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. "
            f"{MAX_COUNT}ê°œì˜ í–¥ìˆ˜ë¥¼ ì—„ì„ í•˜ì—¬ ì¶”ì²œë“œë ¸ìŠµë‹ˆë‹¤.\n\n"
        )

    return ""


def generate_post_notice(requested: int, actual: int, is_explicit: bool) -> str:
    """
    ìŠ¤íŠ¸ë¦¬ë° í›„ì— ì¶œë ¥ ê°€ëŠ¥í•œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    (ì¼€ì´ìŠ¤ 2: ë¶€ë¶„ ì‹¤íŒ¨)

    Args:
        requested: ìš”ì²­ëœ ê°œìˆ˜
        actual: ì‹¤ì œ ìƒì„±ëœ ê°œìˆ˜
        is_explicit: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ê°œìˆ˜ë¥¼ ìš”ì²­í–ˆëŠ”ì§€

    Returns:
        ì•ˆë‚´ ë©”ì‹œì§€ (í•„ìš” ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    """
    # ì¼€ì´ìŠ¤ 2: ë¶€ë¶„ ì‹¤íŒ¨ (ëª…ì‹œì  ìš”ì²­ì¼ ë•Œë§Œ!)
    if is_explicit and actual < requested:
        return (
            f"\nğŸ’¡ ì•ˆë‚´: ìš”ì²­í•˜ì‹  {requested}ê°œ ì¤‘ {actual}ê°œì˜ í–¥ìˆ˜ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. "
            f"ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ê°€ ì œí•œì ì´ì—ˆìŠµë‹ˆë‹¤."
        )

    return ""


async def smart_search_with_retry_async(
    h_filters: dict,
    s_filters: dict,
    exclude_ids: Optional[List[int]] = None,
    query_text: str = "",
    rank_mode: str = "DEFAULT",
):
    _ = advanced_perfume_search_tool  # Keep reference for monkeypatches/tests
    return await smart_perfume_search(
        h_filters=h_filters,
        s_filters=s_filters,
        exclude_ids=exclude_ids,
        query_text=query_text,
        rank_mode=rank_mode,
    )


async def call_info_graph_wrapper(state: AgentState):
    """Sub-Graph Wrapper"""
    current_query = state.get("user_query", "")

    if not current_query and state.get("messages"):
        last_msg = state["messages"][-1]
        if isinstance(last_msg, HumanMessage):
            current_query = last_msg.content

    subgraph_input = {
        "user_query": current_query,
        "messages": state.get("messages", []),
        "user_mode": state.get("user_mode", "BEGINNER"),
    }

    try:
        result = await info_graph.ainvoke(subgraph_input)

        # [Wave 2-4] Map info_status to chat_outcome_status
        info_status = result.get("info_status", "OK")

        return {
            "messages": result.get("messages", []),
            "chat_outcome_status": info_status,  # OK/NO_RESULTS/ERROR ì§ì ‘ ë§¤í•‘
            "chat_outcome_reason_code": f"info_{info_status.lower()}",
            "chat_outcome_reason_detail": f"Info graph completed with status: {info_status}",
        }

    except Exception as e:
        import traceback

        traceback.print_exc()

        # [Wave 2-4] Set ERROR status on exception
        return {
            "messages": [AIMessage(content="ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")],
            "chat_outcome_status": "ERROR",
            "chat_outcome_reason_code": "info_exception",
            "chat_outcome_reason_detail": f"Info graph exception: {str(e)[:100]}",
        }


# ==========================================
# 3. Node Functions
# ==========================================


def pre_validator_node(state: AgentState):
    """
    [Pre-Validator] ìš”ì²­ ì‹¤í˜„ ê°€ëŠ¥ì„± ì‚¬ì „ ê²€ì¦.
    DBì— ì—†ëŠ” ì†ì„± ìš”ì²­ì„ ì¡°ê¸° ì°¨ë‹¨í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 60, flush=True)
    print("ğŸ” [Pre-Validator] ìš”ì²­ ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦ ì¤‘...", flush=True)

    messages = [SystemMessage(content=PRE_VALIDATOR_PROMPT)] + state["messages"]

    try:
        result = SMART_LLM.with_structured_output(ValidationResult).invoke(messages)

        if result.is_unsupported:
            print(
                f"   âŒ ì§€ì› ë¶ˆê°€: {result.unsupported_category} - {result.reason}",
                flush=True,
            )
            return {
                "validation_result": "unsupported",
                "unsupported_category": result.unsupported_category,
                "unsupported_reason": result.reason,
            }
        else:
            print(f"   âœ… ì§€ì› ê°€ëŠ¥ - {result.reason}", flush=True)
            return {"validation_result": "supported"}

    except Exception as e:
        print(f"   âš ï¸ ê²€ì¦ ì‹¤íŒ¨(Error): {e} -> ê¸°ë³¸ê°’ ì§€ì› ê°€ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬", flush=True)
        return {"validation_result": "supported"}


def supervisor_node(state: AgentState):
    """[Main Router]"""
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜ ì¤‘...", flush=True)

    if state.get("active_mode") == "interviewer":
        print("   ğŸ‘‰ ì¸í„°ë·° ì§„í–‰ ì¤‘ -> Interviewerë¡œ ì´ë™", flush=True)
        return {"next_step": "interviewer"}

    messages = [SystemMessage(content=SUPERVISOR_PROMPT)] + state["messages"]

    try:
        decision = SMART_LLM.with_structured_output(RoutingDecision).invoke(messages)
        next_step = decision.next_step
        print(f"   ğŸ‘‰ ë¶„ë¥˜ ê²°ê³¼: {next_step}", flush=True)
        return {"next_step": next_step}

    except Exception as e:
        print(f"   âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨(Error): {e} -> ê¸°ë³¸ê°’ Writerë¡œ ì´ë™", flush=True)
        return {"next_step": "writer"}


def interviewer_node(state: AgentState):
    """[Interviewer]"""
    current_prefs = state.get("user_preferences") or {}
    if isinstance(current_prefs, UserPreferences):
        current_prefs = current_prefs.model_dump(exclude_none=True)
    question_count = state.get("question_count", 0)

    # ì§ˆë¬¸ íšŸìˆ˜ ì¦ê°€
    question_count += 1

    # ê±°ë¶€ í‚¤ì›Œë“œ ê°ì§€
    rejection_keywords = ["ëª°ë¼", "ì•„ë¬´ê±°ë‚˜", "ê·¸ëƒ¥ ì¶”ì²œ", "ë¹¨ë¦¬", "ëª¨ë¥´ê² ", "ìƒê´€ì—†"]
    user_message = state["messages"][-1].content.lower() if state["messages"] else ""
    is_rejection = any(keyword in user_message for keyword in rejection_keywords)

    # ì§ˆë¬¸ ìƒí•œ ë˜ëŠ” ê±°ë¶€ ê°ì§€ ì‹œ í´ë°± íŠ¸ë¦¬ê±°
    should_fallback = (question_count >= 3) or (question_count >= 2 and is_rejection)

    if should_fallback:
        # í´ë°±: ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        fallback_prefs = {
            **current_prefs,
            "gender": current_prefs.get("gender", "Unisex"),
            # season, occasionì€ Noneìœ¼ë¡œ ë‘ì–´ í•„í„°ë§ ì•ˆ í•¨ (ëª¨ë“  ê³„ì ˆ/ìƒí™© í¬í•¨)
            "season": current_prefs.get("season"),
            "occasion": current_prefs.get("occasion"),
            "style": current_prefs.get("style", "Daily"),
            "target": current_prefs.get("target", "ì¼ë°˜"),
        }
        fallback_frame_id = state.get("frame_id") or str(uuid.uuid4())

        print(
            f"      âš ï¸ [Fallback] ì§ˆë¬¸ ìƒí•œ ë„ë‹¬ ë˜ëŠ” ê±°ë¶€ ê°ì§€. ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ì²œ ì§„í–‰: {json.dumps(fallback_prefs, ensure_ascii=False)}",
            flush=True,
        )

        return {
            "next_step": "researcher",
            "user_preferences": fallback_prefs,
            "status": "ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì„ ì¤€ë¹„í•©ë‹ˆë‹¤...",
            "active_mode": None,
            "question_count": question_count,
            "fallback_triggered": True,
            "frame_id": fallback_frame_id,
            "recommended_history": state.get("recommended_history", []),
        }

    # í˜„ì¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    current_context_str = json.dumps(current_prefs, ensure_ascii=False)

    # [â˜…ìˆ˜ì •] ì—¬ê¸°ì„œ CURRENT_CONTEXTë§Œ ì±„ì›Œì£¼ë©´ ë©ë‹ˆë‹¤! (SUFFICIENCY_CRITERIAëŠ” ì´ë¯¸ ë“¤ì–´ìˆìŒ)
    try:
        formatted_prompt = INTERVIEWER_PROMPT.format(
            CURRENT_CONTEXT=current_context_str
        )
    except Exception as e:
        # í˜¹ì‹œë¼ë„ í¬ë§·íŒ… ì—ëŸ¬ê°€ ë‚˜ë©´ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
        print(f"âš ï¸ Prompt Formatting Error: {e}")
        formatted_prompt = INTERVIEWER_PROMPT.replace(
            "{{CURRENT_CONTEXT}}", "ì •ë³´ ì—†ìŒ"
        )

    messages = [SystemMessage(content=formatted_prompt)] + state["messages"]

    try:
        interview_result = SMART_LLM.with_structured_output(InterviewResult).invoke(
            messages
        )

        current_query = state.get("user_query", "")
        recent_messages = state.get("messages", [])[-5:]
        if not current_query and state.get("messages"):
            last_msg = state["messages"][-1]
            if isinstance(last_msg, HumanMessage):
                current_query = last_msg.content

        classification = classify_followup(
            current_query=current_query,
            recent_messages=recent_messages,
            current_constraints=current_prefs,
        )

        # [â˜…ì¶”ê°€] ë¸Œëœë“œ ì œì™¸ íŒŒì‹± (ì„¸ì…˜ ë ˆë²¨ ìœ ì§€)
        session_exclude_brands = state.get("exclude_brands", [])
        current_exclude_brands, has_exclusion = parse_brand_exclusions(current_query)

        # ìƒˆë¡œìš´ ì œì™¸ ìš”ì²­ì´ ìˆìœ¼ë©´ ëˆ„ì 
        if has_exclusion:
            session_exclude_brands = list(
                set(session_exclude_brands + current_exclude_brands)
            )
            print(
                f"ğŸš« [Exclusion] Detected exclude_brands: {current_exclude_brands}, Session: {session_exclude_brands}",
                flush=True,
            )

        # ëª…ì‹œì  ë¸Œëœë“œ ìš”ì²­ ì‹œ í•´ë‹¹ ë¸Œëœë“œ ì œì™¸ ëª©ë¡ì—ì„œ í•´ì œ
        if current_prefs and current_prefs.get("brand"):
            requested_brand = current_prefs.get("brand")
            if requested_brand in session_exclude_brands:
                session_exclude_brands.remove(requested_brand)
                print(
                    f"   ğŸ”„ [Exclusion] Removed {requested_brand} from exclusion list due to explicit request",
                    flush=True,
                )

        current_frame_id = state.get("frame_id")
        if classification.intent in ["NEW_RECO", "RESET"]:
            frame_id = str(uuid.uuid4())
            # [â˜…ìˆ˜ì •] íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€ (ì„¸ì…˜ ë‚´ë‚´ ëˆ„ì )
            new_recommended_history = None  # None = ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ìœ ì§€
            print(
                f"ğŸ”„ [Frame] New frame created: {frame_id[:8]}... (intent={classification.intent})",
                flush=True,
            )
            print(
                "ğŸ“š [History] Recommended history maintained (session-level)",
                flush=True,
            )
            # [â˜…ì œê±°] DB í´ë¦¬ì–´ ì•ˆ í•¨ - ì„¸ì…˜ ë‚´ë‚´ ìœ ì§€
        else:
            frame_id = current_frame_id or str(uuid.uuid4())
            new_recommended_history = None
            print(
                f"âœ… [Frame] Frame maintained: {frame_id[:8] if frame_id else 'new'}... (intent={classification.intent})",
                flush=True,
            )

        merged_prefs: Dict[str, Any] = {}
        for slot in classification.keep_slots:
            if (
                current_prefs
                and slot in current_prefs
                and current_prefs[slot] is not None
            ):
                merged_prefs[slot] = current_prefs[slot]

        new_prefs = interview_result.user_preferences
        for key, value in new_prefs.model_dump(exclude_none=True).items():
            merged_prefs[key] = value

        # [â˜…ì¶”ê°€] ë¸Œëœë“œ ì œì™¸ ì²˜ë¦¬
        merged_prefs["exclude_brands"] = session_exclude_brands
        # ì œì™¸ ë¸Œëœë“œê°€ ìˆìœ¼ë©´ brand, reference_brand í´ë¦¬ì–´
        if should_clear_brand_fields(session_exclude_brands):
            merged_prefs["brand"] = None
            merged_prefs["reference_brand"] = None
            print(
                f"   â†’ brand/reference_brand cleared due to exclusions",
                flush=True,
            )

        for slot in classification.drop_slots:
            if slot not in merged_prefs:
                merged_prefs[slot] = None

        print(
            f"ğŸ“‹ [Merge] Keep: {classification.keep_slots}, Drop: {classification.drop_slots}",
            flush=True,
        )
        print(f"ğŸ“‹ [Merge] Result: {list(merged_prefs.keys())}", flush=True)

        state["user_preferences"] = merged_prefs
        # [â˜…ì¶”ê°€] recommended_countë¥¼ state ìµœìƒìœ„ë¡œ ì˜¬ë¦¼ (parallel_reco_nodeì—ì„œ ì‚¬ìš©)
        if merged_prefs.get("recommended_count"):
            state["recommended_count"] = merged_prefs["recommended_count"]
            state["is_count_explicit"] = True
        else:
            state["is_count_explicit"] = False

        if interview_result.is_sufficient:
            print(
                f"      âœ… [Handover] ì •ë³´ í™•ë³´ ì™„ë£Œ! Researcherë¡œ ì „ë‹¬: {json.dumps(merged_prefs, ensure_ascii=False)}",
                flush=True,
            )
            return {
                "next_step": "researcher",
                "user_preferences": merged_prefs,
                "recommended_count": merged_prefs.get(
                    "recommended_count"
                ),  # [â˜…ìˆ˜ì •] ë°˜í™˜ê°’ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                "is_count_explicit": merged_prefs.get("recommended_count")
                is not None,  # [â˜…ì¶”ê°€] ëª…ì‹œì  ìš”ì²­ ì—¬ë¶€
                "status": "ëª¨ë“  ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤...",
                "active_mode": None,
                "question_count": question_count,
                "fallback_triggered": False,
                "frame_id": frame_id,
                "recommended_history": (
                    new_recommended_history
                    if new_recommended_history is not None
                    else state.get("recommended_history", [])
                ),
                "exclude_brands": session_exclude_brands,  # [â˜…ì¶”ê°€] ì„¸ì…˜ ë ˆë²¨ ì œì™¸ ë¸Œëœë“œ ìœ ì§€
            }

        return {
            "messages": [AIMessage(content=interview_result.response_message)],
            "user_preferences": merged_prefs,
            "recommended_count": merged_prefs.get(
                "recommended_count"
            ),  # [â˜…ìˆ˜ì •] ë°˜í™˜ê°’ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
            "is_count_explicit": merged_prefs.get("recommended_count")
            is not None,  # [â˜…ì¶”ê°€] ëª…ì‹œì  ìš”ì²­ ì—¬ë¶€
            "active_mode": "interviewer",
            "next_step": "end",
            "question_count": question_count,
            "fallback_triggered": False,
            "frame_id": frame_id,
            "recommended_history": (
                new_recommended_history
                if new_recommended_history is not None
                else state.get("recommended_history", [])
            ),
            "exclude_brands": session_exclude_brands,  # [â˜…ì¶”ê°€] ì„¸ì…˜ ë ˆë²¨ ì œì™¸ ë¸Œëœë“œ ìœ ì§€
        }
    except Exception as e:
        print(f"Interviewer Error: {e}")
        return {
            "next_step": "writer",
            "question_count": question_count,
            "fallback_triggered": False,
        }


# ==========================================
# [REMOVED] Old researcher_node and writer_node
# These have been replaced by parallel_reco_node which consolidates
# both functionalities with FCFS streaming.
# ==========================================


def _normalize_section_boundary(previous_text: str, next_text: str) -> str:
    if not previous_text or not next_text:
        return next_text
    if not next_text.lstrip().startswith("##"):
        return next_text
    prev_trimmed = previous_text.rstrip()
    if prev_trimmed.endswith("---") and not previous_text.endswith("\n"):
        if not next_text.startswith("\n"):
            return f"\n{next_text}"
    return next_text


def _merge_unique_ids(*iterables: List[int]) -> List[int]:
    merged: List[int] = []
    seen: Set[int] = set()
    for iterable in iterables:
        if not iterable:
            continue
        for value in iterable:
            if value is None:
                continue
            if value in seen:
                continue
            seen.add(value)
            merged.append(value)
    return merged


def _extract_saved_ids(messages: List[Any]) -> List[int]:
    save_refs = extract_save_refs(messages or [])
    saved_ids: List[int] = []
    for ref in save_refs:
        value = ref.get("id")
        if isinstance(value, int):
            saved_ids.append(value)
    return saved_ids


class RecoSearcher:
    def __init__(
        self,
        *,
        member_id: int,
        user_prefs: Dict[str, Any],
        researcher_prompt: str,
        plan_llm: Any,
        session_exclude_ids: Set[int],
        selection_lock: asyncio.Lock,
        batch_selected_ids: Set[int],
        brand_counts: Dict[str, int],
        search_fn: Any,
    ) -> None:
        self.member_id = member_id
        self.user_prefs = user_prefs
        self.current_context = json.dumps(user_prefs, ensure_ascii=False)
        self.researcher_prompt = researcher_prompt
        self.plan_llm = plan_llm
        self.session_exclude_ids = session_exclude_ids
        self.selection_lock = selection_lock
        self.batch_selected_ids = batch_selected_ids
        self.brand_counts = brand_counts
        self.search_fn = search_fn
        self.user_requested_brand = bool(
            user_prefs.get("brand") or user_prefs.get("reference_brand")
        )

    async def generate_user_label(self, plan_reason: str) -> str:
        """
        Generate user-friendly strategy label using LLM with denylist validation.

        Args:
            plan_reason: Strategy reason/intent
            plan_strategy_name: Internal strategy name (for context only)

        Returns:
            User-friendly label string
        """
        user_prefs_str = json.dumps(self.user_prefs, ensure_ascii=False)

        label_messages = [
            SystemMessage(
                content="ë‹¹ì‹ ì€ í–¥ìˆ˜ ì¶”ì²œ ì „ëµì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í•œ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            ),
            HumanMessage(
                content=(
                    f"ì‚¬ìš©ì ì •ë³´: {user_prefs_str}\n"
                    f"ì „ëµ ì˜ë„: {plan_reason}\n\n"
                    "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì „ëµëª…ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
                    "ìš”êµ¬ì‚¬í•­:\n"
                    '- í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„± (ì˜ˆ: "ê°•ì¸í•˜ê³  ìì‹ ê° ìˆëŠ” ì²«ì¸ìƒ", "ìš°ì•„í•˜ê³  ì„¸ë ¨ëœ ë¶„ìœ„ê¸°")\n'
                    "- ì²«ì¸ìƒ/ë¬´ë“œ ì¤‘ì‹¬ í‘œí˜„ ì‚¬ìš©\n"
                    "- ë‹¤ìŒ ë‹¨ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€: ì „ëµ, ì „ëµì , ì´ë¯¸ì§€ ê°•ì¡°, ì´ë¯¸ì§€ ë³´ì™„, ì´ë¯¸ì§€ ë°˜ì „\n\n"
                    "ì „ëµëª…:"
                )
            ),
        ]

        try:
            response = await SMART_LLM.ainvoke(
                label_messages, config={"tags": ["internal_helper"]}
            )
            user_label = response.content.strip()

            if not has_forbidden_words(user_label):
                return user_label

            retry_messages = [
                SystemMessage(
                    content="ë‹¹ì‹ ì€ í–¥ìˆ˜ ì¶”ì²œ ì „ëµì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í•œ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                ),
                HumanMessage(
                    content=(
                        "ì´ì „ ì‘ë‹µì— ê¸ˆì§€ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n"
                        "ë‹¤ì‹œ í•œë²ˆ ì‘ì„±í•˜ì„¸ìš”. ì ˆëŒ€ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ë‹¨ì–´: ì „ëµ, ì „ëµì , ì´ë¯¸ì§€ ê°•ì¡°, ì´ë¯¸ì§€ ë³´ì™„, ì´ë¯¸ì§€ ë°˜ì „\n\n"
                        f"ì‚¬ìš©ì ì •ë³´: {user_prefs_str}\n"
                        f"ì „ëµ ì˜ë„: {plan_reason}\n\n"
                        "ì „ëµëª…:"
                    )
                ),
            ]

            retry_response = await SMART_LLM.ainvoke(
                retry_messages, config={"tags": ["internal_helper"]}
            )
            user_label = retry_response.content.strip()

            if not has_forbidden_words(user_label):
                return user_label
        except Exception as e:
            print(f"[WARNING] User label generation failed: {e}", flush=True)

        return random.choice(UserFriendlyStrategyLabels.SAFE_LABELS)

    async def _snapshot_exclude_ids(self) -> List[int]:
        async with self.selection_lock:
            batch_ids = set(self.batch_selected_ids)
        return list(self.session_exclude_ids | batch_ids)

    async def _select_candidate(
        self, candidates: List[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        async with self.selection_lock:
            for candidate in candidates:
                candidate_id = candidate.get("id")
                if candidate_id is None:
                    continue
                try:
                    perfume_id = int(candidate_id)
                except (TypeError, ValueError):
                    continue
                brand = candidate.get("brand", "")
                if perfume_id in self.session_exclude_ids:
                    continue
                if perfume_id in self.batch_selected_ids:
                    continue
                if not self.user_requested_brand:
                    if self.brand_counts.get(brand, 0) >= 2:
                        continue
                self.batch_selected_ids.add(perfume_id)
                self.brand_counts[brand] = self.brand_counts.get(brand, 0) + 1
                candidate = dict(candidate)
                candidate["id"] = perfume_id
                return candidate
        return None

    async def _run_search(
        self,
        h_filters: Dict[str, Any],
        s_filters: Dict[str, Any],
        *,
        exclude_ids: List[int],
        query_text: str,
        rank_mode: str,
    ) -> Any:
        try:
            return await self.search_fn(
                h_filters,
                s_filters,
                exclude_ids=exclude_ids,
                query_text=query_text,
                rank_mode=rank_mode,
            )
        except TypeError as e:
            if "rank_mode" not in str(e):
                raise
            return await self.search_fn(
                h_filters,
                s_filters,
                exclude_ids=exclude_ids,
                query_text=query_text,
            )

    async def prepare_strategy(
        self, strategy_name: str, priority: int, rank_mode: str
    ) -> Dict[str, Any]:
        plan_messages = [
            SystemMessage(content=self.researcher_prompt),
            HumanMessage(
                content=(
                    f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {self.current_context}\n"
                    f"ì „ëµ ì´ë¦„: {strategy_name}\n"
                    f"ìš°ì„ ìˆœìœ„: {priority}\n"
                    "ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ëµì„ ìˆ˜ë¦½í•´ ì£¼ì„¸ìš”."
                )
            ),
        ]

        try:
            plan = await self.plan_llm.ainvoke(
                plan_messages, config={"tags": ["internal_helper"]}
            )
        except Exception as e:
            return {
                "error": True,
                "error_type": "llm_error",
                "error_detail": str(e),
                "section_data": None,
                "priority": priority,
            }

        user_label = await self.generate_user_label(plan.reason)

        try:
            h_filters = plan.hard_filters.model_dump(exclude_none=True)
            s_filters = plan.strategy_filters.model_dump(exclude_none=True)
        except Exception:
            h_filters = {}
            s_filters = {}

        try:
            exclude_ids = await self._snapshot_exclude_ids()
            # ë¡œê·¸: ì „ëµë³„ ê²€ìƒ‰ ì‹œ ì‚¬ìš©ë˜ëŠ” ì œì™¸ ID
            print(
                f"   ğŸ” [Strategy {priority}] Searching with {len(exclude_ids)} exclusions",
                flush=True,
            )
            candidates, _match_type = await self._run_search(
                h_filters,
                s_filters,
                exclude_ids=exclude_ids,
                query_text=plan.reason,
                rank_mode=rank_mode,
            )
        except Exception as e:
            return {
                "error": True,
                "error_type": "tool_error",
                "error_detail": str(e),
                "section_data": None,
                "priority": priority,
            }

        if not candidates:
            return {
                "error": True,
                "error_type": "no_results",
                "error_detail": "No candidates returned",
                "section_data": None,
                "priority": priority,
            }

        selected_perfume = await self._select_candidate(candidates)

        # ë¡œê·¸: ì„ íƒëœ í–¥ìˆ˜
        if selected_perfume:
            print(
                f"   âœ… [Strategy {priority}] Selected perfume ID: {selected_perfume.get('id')}",
                flush=True,
            )

        if not selected_perfume:
            try:
                exclude_ids = await self._snapshot_exclude_ids()
                # ë¡œê·¸: ì¬ì‹œë„ ì‹œ ì œì™¸ ID
                print(
                    f"   ğŸ”„ [Strategy {priority}] Retry with {len(exclude_ids)} exclusions",
                    flush=True,
                )
                candidates, _match_type = await self._run_search(
                    h_filters,
                    s_filters,
                    exclude_ids=exclude_ids,
                    query_text=plan.reason,
                    rank_mode=rank_mode,
                )
            except Exception as e:
                return {
                    "error": True,
                    "error_type": "tool_error",
                    "error_detail": str(e),
                    "section_data": None,
                    "priority": priority,
                }
            selected_perfume = await self._select_candidate(candidates)

            # ë¡œê·¸: ì¬ì‹œë„ í›„ ì„ íƒëœ í–¥ìˆ˜
            if selected_perfume:
                print(
                    f"   âœ… [Strategy {priority}] Selected perfume ID (retry): {selected_perfume.get('id')}",
                    flush=True,
                )

        if not selected_perfume:
            return {
                "error": True,
                "error_type": "no_candidates",
                "error_detail": "No candidates selected",
                "section_data": None,
                "priority": priority,
            }

        save_recommendation_log(
            member_id=self.member_id,
            perfumes=[selected_perfume],
            reason=plan.reason,
        )

        perfume_id = int(selected_perfume["id"])
        perfume_name = selected_perfume.get("name") or selected_perfume.get(
            "perfume_name"
        )
        perfume_brand = selected_perfume.get("brand") or selected_perfume.get(
            "perfume_brand"
        )
        perfume_name = str(perfume_name) if perfume_name is not None else "Unknown"
        perfume_brand = str(perfume_brand) if perfume_brand is not None else "Unknown"

        accords_text = selected_perfume.get("accords") or ""
        best_review = selected_perfume.get("best_review") or ""
        accord_value = f"{accords_text}\n[Best Review]: {best_review}".strip()

        strategy_result = StrategyResult(
            strategy_name=plan.strategy_name,
            strategy_keyword=plan.strategy_keyword,
            strategy_reason=plan.reason,
            perfumes=[
                PerfumeDetail(
                    id=perfume_id,
                    perfume_name=perfume_name,
                    perfume_brand=perfume_brand,
                    accord=accord_value,
                    notes=PerfumeNotes(
                        top=selected_perfume.get("top_notes") or "N/A",
                        middle=selected_perfume.get("middle_notes") or "N/A",
                        base=selected_perfume.get("base_notes") or "N/A",
                    ),
                    image_url=selected_perfume.get("image_url"),
                    gender=selected_perfume.get("gender", "Unisex"),
                    season=selected_perfume.get("seasons") or "All",
                    occasion=selected_perfume.get("occasions") or "Any",
                )
            ],
        )

        section_data = {
            "user_preferences": self.user_prefs,
            "strategy": {
                "internal_id": plan.strategy_name,
                "user_label": user_label,
                "reason": plan.reason,
                "keywords": plan.strategy_keyword,
                "priority": priority,
            },
            "perfume": strategy_result.perfumes[0].dict(),
        }

        return {
            "section_data": section_data,
            "priority": priority,
            "perfume_id": perfume_id,
        }


class RecoWriter:
    def __init__(self, state: AgentState) -> None:
        self.state = state
        self.user_mode = state.get("user_mode", "BEGINNER")

    def _build_expression_text(self, section_data: Dict[str, Any]) -> str:
        perfume_data = section_data.get("perfume", {})
        notes_data = perfume_data.get("notes", {})
        accord_str = perfume_data.get("accord", "")

        all_notes: List[str] = []
        for note_type in ["top", "middle", "base"]:
            note_str = notes_data.get(note_type, "")
            if note_str and note_str != "N/A":
                all_notes.extend([n.strip() for n in note_str.split(",")])

        accords: List[str] = []
        if accord_str:
            accord_part = accord_str.split("[Best Review]")[0].strip()
            accords = [a.strip() for a in accord_part.split(",") if a.strip()]

        loader = ExpressionLoader()

        expression_guide: List[str] = []

        if all_notes:
            expression_guide.append("### ë…¸íŠ¸ í‘œí˜„ ê°€ì´ë“œ")
            for note in all_notes[:10]:
                desc = loader.get_note_desc(note)
                if desc:
                    expression_guide.append(f"- {note}: {desc}")

        if accords:
            expression_guide.append("\n### ì–´ì½”ë“œ í‘œí˜„ ê°€ì´ë“œ")
            for accord in accords[:10]:
                desc = loader.get_accord_desc(accord)
                if desc:
                    expression_guide.append(f"- {accord}: {desc}")

        return "\n".join(expression_guide) if expression_guide else ""

    async def generate_section(
        self,
        prepared_data: Dict[str, Any],
        display_priority: int,
        *,
        is_first: bool,
        is_last: bool,
    ) -> Optional[str]:
        if not prepared_data:
            return None

        section_data = prepared_data.get("section_data")
        if not section_data:
            return None

        expression_text = self._build_expression_text(section_data)

        data_ctx = json.dumps(section_data, ensure_ascii=False, indent=2)

        if self.user_mode == "EXPERT":
            section_system = WRITER_RECOMMENDATION_PROMPT_EXPERT_SINGLE
        else:
            section_system = WRITER_RECOMMENDATION_PROMPT_SINGLE

        content_parts = [
            f"[ì„¹ì…˜ ë²ˆí˜¸]: {display_priority}",
            f"[ë„ì…ë¶€ í¬í•¨]: {'ì˜ˆ' if is_first else 'ì•„ë‹ˆì˜¤'}",
            f"[ë§ˆì§€ë§‰ ì„¹ì…˜ ì—¬ë¶€]: {'ì˜ˆ' if is_last else 'ì•„ë‹ˆì˜¤'}",
            (
                f"[ì¶œë ¥ ê·œì¹™]: ë„ì…ë¶€ í¬í•¨ì´ 'ì•„ë‹ˆì˜¤'ì´ë©´ ì²« ì¤„ì„ ë°˜ë“œì‹œ '## {display_priority}.'ë¡œ ì‹œì‘í•˜ê³  ë„ì…ë¶€ ë¬¸ì¥ì„ ì“°ì§€ ë§ˆì„¸ìš”."
            ),
        ]

        # [â˜…ì¶”ê°€] ë§ˆì§€ë§‰ ì„¹ì…˜ì¼ ë•Œ ì¢…í•© ì˜ê²¬ ì¶”ê°€ ì§€ì‹œ
        if is_last:
            content_parts.append(
                "\n[ë§ˆì§€ë§‰ ì„¹ì…˜ ì§€ì‹œì‚¬í•­]: ì´ ì„¹ì…˜ì´ ë§ˆì§€ë§‰ì´ë¯€ë¡œ, í–¥ìˆ˜ ì„¤ëª…ê³¼ [[SAVE:...]] íƒœê·¸ë¥¼ ëª¨ë‘ ì‘ì„±í•œ í›„ "
                "êµ¬ë¶„ì„ (---)ì„ ì¶”ê°€í•˜ê³ , ê·¸ ì•„ë˜ì— í–¥ìˆ˜ ì‚¬ìš©ì— ëŒ€í•œ ì¹œì ˆí•œ ì¢…í•© ì˜ê²¬ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
                "ì˜ˆ: 'ë§ˆì§€ë§‰ìœ¼ë¡œ, í–¥ì„ ì²˜ìŒ ë“¤ì´ì‹¤ ë• 1~2ë²ˆë§Œ ê°€ë³ê²Œ ë¿Œë ¤ì„œ ë‚´ ì‚´ê²°ì— ì–´ë–»ê²Œ ë‚¨ëŠ”ì§€ë¶€í„° í™•ì¸í•´ë³´ì„¸ìš”. "
                'ë°ì¼ë¦¬ì¼ìˆ˜ë¡ "ê³¼í•˜ì§€ ì•Šì€ ì”í–¥"ì´ ê°€ì¥ ì˜¤ë˜ ê°‘ë‹ˆë‹¤.\''
            )

        if expression_text:
            content_parts.append(f"\n[ê°ê° í‘œí˜„ ì°¸ê³ ]:\n{expression_text}")

        content_parts.append(f"\n[ì°¸ê³  ë°ì´í„°]:\n{data_ctx}")

        messages = (
            [SystemMessage(content=section_system)]
            + self.state.get("messages", [])
            + [HumanMessage(content="\n".join(content_parts))]
        )

        try:
            result_text = ""
            if hasattr(SUPER_SMART_LLM, "astream"):
                async for chunk in SUPER_SMART_LLM.astream(messages):
                    if chunk.content:
                        result_text += chunk.content
            else:
                response = await SUPER_SMART_LLM.ainvoke(messages)
                result_text = response.content or ""

            if result_text:
                header_index = result_text.find("##")
                if display_priority != 1 and header_index > 0:
                    result_text = result_text[header_index:]
                if result_text.startswith("##"):
                    lines = result_text.splitlines()
                    header_line = lines[0]
                    after = header_line[2:].lstrip()
                    idx = 0
                    while idx < len(after) and after[idx].isdigit():
                        idx += 1
                    if idx < len(after) and after[idx] == ".":
                        idx += 1
                    if idx < len(after) and after[idx] == " ":
                        idx += 1
                    rest = after[idx:]
                    lines[0] = (
                        f"## {display_priority}. {rest}"
                        if rest
                        else f"## {display_priority}."
                    )
                    result_text = "\n".join(lines)
            if result_text and not result_text.rstrip().endswith("---"):
                result_text = f"{result_text.rstrip()}\n---"
            return result_text
        except Exception as e:
            logger.error(f"Writer error: {e}")
            return None


async def parallel_reco_node(state: AgentState):
    member_id = state.get("member_id", 0)
    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)

    use_case = infer_use_case(user_prefs)

    personalization = {}
    if use_case == "SELF" and member_id > 0:
        personalization = get_personalization_summary(member_id) or {}
        if personalization.get("summary_text"):
            print(f"ğŸ¯ [Personalization] {personalization['summary_text']}", flush=True)
    else:
        if use_case == "GIFT":
            print(
                "ğŸ [GIFT Mode] Personalization disabled for gift recommendations",
                flush=True,
            )

    researcher_prompt = RESEARCHER_SYSTEM_PROMPT
    if personalization.get("summary_text"):
        researcher_prompt += (
            "\n\n## ì‚¬ìš©ì ì·¨í–¥ ì •ë³´\n"
            f"{personalization['summary_text']}\n\n"
            "ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ìš”ì²­ ì¡°ê±´(ë¸Œëœë“œ/ê³„ì ˆ/ëŒ€ìƒ ë“±)ì´ ìµœìš°ì„ ì…ë‹ˆë‹¤."
        )

    plan_llm = SMART_LLM.with_structured_output(SearchStrategyPlan)

    recommended_history = state.get("recommended_history") or []
    saved_ids = _extract_saved_ids(state.get("messages", []))
    if not recommended_history and saved_ids:
        recommended_history = saved_ids
    merged_history = _merge_unique_ids(recommended_history, saved_ids)

    session_exclude_ids: Set[int] = set(merged_history)

    # ë¡œê·¸: íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì œì™¸ ID
    if session_exclude_ids:
        print(
            f"ğŸš« [Exclude] History-based exclusions: {sorted(list(session_exclude_ids))}",
            flush=True,
        )

    if use_case == "SELF":
        disliked_ids = []
        for disliked in personalization.get("disliked_perfumes", []):
            perfume_id = disliked.get("id")
            if perfume_id:
                session_exclude_ids.add(perfume_id)
                disliked_ids.append(perfume_id)

        # ë¡œê·¸: ì‹«ì–´í•˜ëŠ” í–¥ìˆ˜ ì œì™¸ ID
        if disliked_ids:
            print(f"ğŸš« [Exclude] Disliked perfumes: {sorted(disliked_ids)}", flush=True)

    # ë¡œê·¸: ìµœì¢… ì œì™¸ ID ì´í•©
    if session_exclude_ids:
        print(
            f"ğŸš« [Exclude] Total session exclusions: {len(session_exclude_ids)} IDs",
            flush=True,
        )
    else:
        print(f"âœ… [Exclude] No exclusions for this session", flush=True)

    selection_lock = asyncio.Lock()
    batch_selected_ids: Set[int] = set()
    brand_counts: Dict[str, int] = {}

    rank_mode = "DEFAULT"
    user_query = state.get("user_query", "")
    trending_keywords = [
        "ìœ í–‰",
        "ì¸ê¸°",
        "íŠ¸ë Œë”©",
        "ìš”ì¦˜",
        "ì˜ë‚˜ê°€ëŠ”",
        "ë² ìŠ¤íŠ¸",
        "trending",
        "popular",
        "hot",
    ]
    if any(k in user_query for k in trending_keywords):
        rank_mode = "POPULAR"
        print(f"ğŸ”¥ [Ranking] Mode: {rank_mode}", flush=True)

    requested_count = state.get("recommended_count")
    if requested_count is None:
        parsed = parse_recommended_count(state.get("user_query", ""))
        requested_count = parsed if parsed is not None else 3

    target_count = normalize_recommended_count(requested_count)

    print(f"ğŸ”¢ [Count] Target recommendations: {target_count}", flush=True)

    searcher = RecoSearcher(
        member_id=member_id,
        user_prefs=user_prefs,
        researcher_prompt=researcher_prompt,
        plan_llm=plan_llm,
        session_exclude_ids=session_exclude_ids,
        selection_lock=selection_lock,
        batch_selected_ids=batch_selected_ids,
        brand_counts=brand_counts,
        search_fn=smart_search_with_retry_async,
    )
    writer = RecoWriter(state)

    prep_tasks = [
        asyncio.create_task(searcher.prepare_strategy(f"STRAT_{i}", i, rank_mode))
        for i in range(1, target_count + 1)
    ]

    errors_encountered: List[Dict[str, str]] = []
    pending_result: Optional[Dict[str, Any]] = None
    output_texts: List[str] = []
    prepared_data_list: List[Dict[str, Any]] = []

    for future in asyncio.as_completed(prep_tasks):
        try:
            result = await future
        except Exception as e:
            errors_encountered.append({"type": "exception", "detail": str(e)})
            continue

        if not result:
            errors_encountered.append(
                {"type": "unknown", "detail": "Strategy returned empty result"}
            )
            continue

        if result.get("error"):
            error_type = result.get("error_type", "unknown")
            error_detail = result.get("error_detail", "")
            if error_type not in {"no_results", "no_candidates"}:
                errors_encountered.append({"type": error_type, "detail": error_detail})
            continue

        if pending_result:
            section_number = len(output_texts) + 1
            output_text = await writer.generate_section(
                pending_result,
                section_number,
                is_first=section_number == 1,
                is_last=False,
            )
            if output_text:
                output_texts.append(output_text)
                prepared_data_list.append(pending_result)
            else:
                errors_encountered.append(
                    {
                        "type": "writer_error",
                        "detail": "Writer failed to produce output",
                    }
                )

        pending_result = result

    if pending_result:
        section_number = len(output_texts) + 1
        output_text = await writer.generate_section(
            pending_result,
            section_number,
            is_first=section_number == 1,
            is_last=True,
        )
        if output_text:
            output_texts.append(output_text)
            prepared_data_list.append(pending_result)
        else:
            errors_encountered.append(
                {
                    "type": "writer_error",
                    "detail": "Writer failed to produce output",
                }
            )

    if output_texts:
        full_text = output_texts[0]
        for next_text in output_texts[1:]:
            next_text = _normalize_section_boundary(full_text, next_text)
            full_text = f"{full_text}\n\n{next_text}"

        # [â˜…ì¶”ê°€] ìŠ¤íŠ¸ë¦¬ë° í›„ ì•ˆë‚´ ë©”ì‹œì§€ (ì¼€ì´ìŠ¤ 1 + ì¼€ì´ìŠ¤ 2)
        actual_count = len(output_texts)
        is_explicit = state.get("is_count_explicit", False)

        # ì¼€ì´ìŠ¤ 1: ê³¼ë‹¤ ìš”ì²­ ë©”ì‹œì§€
        pre_notice_msg = generate_pre_notice(requested_count, is_explicit)
        # ì¼€ì´ìŠ¤ 2: ë¶€ë¶„ ì‹¤íŒ¨ ë©”ì‹œì§€
        post_notice_msg = generate_post_notice(
            requested_count, actual_count, is_explicit
        )

        # í•˜ë‚˜ë§Œ ì„ íƒí•´ì„œ ì¶”ê°€ (ìš°ì„ ìˆœìœ„: ì¼€ì´ìŠ¤ 1 > ì¼€ì´ìŠ¤ 2)
        notice_msg = ""
        if pre_notice_msg:
            # ì¼€ì´ìŠ¤ 1: ê³¼ë‹¤ ìš”ì²­ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            notice_msg = pre_notice_msg.strip()
        elif post_notice_msg:
            # ì¼€ì´ìŠ¤ 2: ë¶€ë¶„ ì‹¤íŒ¨ (ê³¼ë‹¤ ìš”ì²­ì´ ì•„ë‹ ë•Œë§Œ)
            notice_msg = post_notice_msg.strip()

        # [â˜…ìˆ˜ì •] ì¢…í•© ì˜ê²¬ì€ LLMì´ ë§ˆì§€ë§‰ ì„¹ì…˜ì—ì„œ ìë™ ìƒì„± (í•˜ë“œì½”ë”© ì œê±°)
        # ì•ˆë‚´ ë©”ì‹œì§€ë§Œ ì¡°ê±´ë¶€ë¡œ ì¶”ê°€
        if notice_msg:
            full_text = f"{full_text}\n{notice_msg}"
            print(f"ğŸ’¬ [Notice] Added notice to end of response", flush=True)
    else:
        fallback_messages = [
            SystemMessage(content=WRITER_FAILURE_PROMPT),
            HumanMessage(content=f"ì‚¬ìš©ì ì •ë³´: {current_context}"),
        ]
        fallback_response = await SUPER_SMART_LLM.ainvoke(fallback_messages)
        full_text = fallback_response.content

    if len(output_texts) >= 1:
        chat_outcome_status = "OK"
        if len(output_texts) < target_count:
            chat_outcome_reason_code = "partial_results"
            chat_outcome_reason_detail = (
                f"Generated {len(output_texts)}/{target_count} sections"
            )
        else:
            chat_outcome_reason_code = "success"
            chat_outcome_reason_detail = (
                f"Generated {len(output_texts)}/{target_count} sections"
            )
    elif errors_encountered:
        chat_outcome_status = "ERROR"
        chat_outcome_reason_code = errors_encountered[0]["type"]
        chat_outcome_reason_detail = (
            f"{len(errors_encountered)} errors: {errors_encountered[0]['detail'][:100]}"
        )
    else:
        chat_outcome_status = "NO_RESULTS"
        chat_outcome_reason_code = "no_candidates"
        chat_outcome_reason_detail = "All strategies failed to find suitable perfumes"

    current_batch_ids: List[int] = []
    for prepared_data in prepared_data_list:
        perfume_id = prepared_data.get("perfume_id")
        if perfume_id:
            current_batch_ids.append(perfume_id)

    # ë¡œê·¸: ì´ë²ˆ ë°°ì¹˜ì—ì„œ ì¶”ì²œëœ í–¥ìˆ˜ IDë“¤
    if current_batch_ids:
        print(
            f"âœ¨ [Batch] Recommended perfume IDs in this batch: {current_batch_ids}",
            flush=True,
        )

    updated_history = _merge_unique_ids(merged_history, current_batch_ids)

    # ë¡œê·¸: ì—…ë°ì´íŠ¸ëœ ì „ì²´ íˆìŠ¤í† ë¦¬
    if updated_history != merged_history:
        print(
            f"ğŸ“š [History] Updated total history: {len(updated_history)} IDs",
            flush=True,
        )

    # [â˜…ì¶”ê°€] DBì— recommended_history ì €ì¥ (thread_id ì•ˆì „ì„± ê²€ì¦)
    thread_id = state.get("thread_id")
    if thread_id and current_batch_ids:
        from .database import update_recommended_history

        try:
            update_recommended_history(thread_id, current_batch_ids, max_size=100)
        except Exception as e:
            print(f"   âš ï¸ [DB] Failed to save recommended_history: {e}", flush=True)
            # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ stateì˜ recommended_historyëŠ” ìœ ì§€ë¨ (ë©”ëª¨ë¦¬ fallback)

    return {
        "messages": [AIMessage(content=full_text)],
        "next_step": "end",
        "recommended_history": updated_history,
        "user_preferences": user_prefs,
        "chat_outcome_status": chat_outcome_status,
        "chat_outcome_reason_code": chat_outcome_reason_code,
        "chat_outcome_reason_detail": chat_outcome_reason_detail,
    }


def parallel_reco_result_router(state: AgentState):
    """
    chat_outcome_status ê°’ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

    Returns:
        ë‹¤ìŒ ë…¸ë“œ ì´ë¦„ ('parallel_reco_ok_writer' | 'parallel_reco_no_results' | 'parallel_reco_error')
    """
    status = state.get("chat_outcome_status", "OK")

    print(f"\nğŸ”€ [Reco Router] Status: {status}", flush=True)

    if status == "NO_RESULTS":
        return "parallel_reco_no_results"
    elif status == "ERROR":
        return "parallel_reco_error"
    else:
        return "parallel_reco_ok_writer"


async def parallel_reco_ok_writer(_state: AgentState):
    """
    OK ìƒíƒœì¼ ë•Œ - ì´ë¯¸ parallel_reco_nodeì—ì„œ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ.
    ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    print(f"\nâœ… [Reco OK Writer] ì •ìƒ ì¶”ì²œ ì™„ë£Œ", flush=True)
    return {}


async def parallel_reco_no_results(state: AgentState):
    """
    NO_RESULTS ìƒíƒœì¼ ë•Œ - WRITER_FAILURE_PROMPT ì‚¬ìš©í•˜ì—¬ ëŒ€ì•ˆ ì œì‹œ.
    """
    print(f"\nâš ï¸ [Reco No Results] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì²˜ë¦¬", flush=True)

    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)

    fallback_messages = [
        SystemMessage(content=WRITER_FAILURE_PROMPT),
        HumanMessage(content=f"ì‚¬ìš©ì ì •ë³´: {current_context}"),
    ]

    fallback_response = await SUPER_SMART_LLM.ainvoke(fallback_messages)

    return {"messages": [AIMessage(content=fallback_response.content)]}


async def parallel_reco_error(_state: AgentState):
    """
    ERROR ìƒíƒœì¼ ë•Œ - ê³ ì • ë¬¸êµ¬ ì¶œë ¥ (ë‚´ë¶€ ì˜¤ë¥˜ ë…¸ì¶œ ê¸ˆì§€).
    """
    print(f"\nâŒ [Reco Error] ê¸°ìˆ ì  ì˜¤ë¥˜ ì²˜ë¦¬", flush=True)

    error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ğŸ™"

    return {"messages": [AIMessage(content=error_msg)]}


async def out_of_scope_handler(_state: AgentState):
    """
    í–¥ìˆ˜ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ ì²˜ë¦¬ - ê³ ì • ë©”ì‹œì§€ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ).
    """
    print(f"\nğŸš« [Out of Scope] í–¥ìˆ˜ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ ì²˜ë¦¬", flush=True)

    fixed_msg = "ì£„ì†¡í•˜ì§€ë§Œ ì €ëŠ” í–¥ìˆ˜ íë ˆì´ì…˜ ì±—ë´‡ì´ê¸° ë•Œë¬¸ì— í–¥ìˆ˜ ì¶”ì²œì´ë‚˜ ì •ë³´ì œê³µ ì´ì™¸ì˜ ë‹µë³€ì„ ë“œë¦¬ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤."

    return {
        "messages": [AIMessage(content=fixed_msg)],
        "chat_outcome_status": "OUT_OF_SCOPE",
    }


async def unsupported_request_handler(_state: AgentState):
    """
    DBì— ì—†ëŠ” ì†ì„± ìš”ì²­ ì²˜ë¦¬ - ì¹´í…Œê³ ë¦¬ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§•ëœ ê³ ì • ë©”ì‹œì§€ ë°˜í™˜.
    """
    print(f"\nâš ï¸ [Unsupported Request] DB ë¯¸ì§€ì› ì†ì„± ìš”ì²­ ì²˜ë¦¬", flush=True)

    category = _state.get("unsupported_category", "")

    # ì¹´í…Œê³ ë¦¬ë³„ ë©”ì‹œì§€ (ì´ìœ  + ëŒ€ì•ˆ)
    category_messages = {
        "ì œí˜•": {
            "reason": "ì›Œí„° í¼í“¸, ì˜¤ì¼ í¼í“¸, ê³ ì²´ í–¥ìˆ˜ ë“± ì œí˜•ë³„ ê²€ìƒ‰ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ëŒ€ì‹  ì›í•˜ì‹œëŠ” ëŠë‚Œ(ê°€ë²¼ìš´, ì‹œì›í•œ, ë¬¼ê¸° ìˆëŠ” ë“±)ì„ ë§ì”€í•´ì£¼ì‹œë©´ ë¹„ìŠ·í•œ í–¥ìˆ˜ë¥¼ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        },
        "ì„±ëŠ¥": {
            "reason": "ë°œí–¥ë ¥, ì§€ì†ë ¥, ì”í–¥ ë“± ì„±ëŠ¥ ì •ë³´ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤.",
            "alternative": "ëŒ€ì‹  ê³„ì ˆì´ë‚˜ ìƒí™©ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì¶”ì²œí•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
        "ê°€ê²©": {
            "reason": "ê°€ê²©ëŒ€ë³„ ê²€ìƒ‰ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ë¸Œëœë“œë‚˜ ë¶„ìœ„ê¸°ë¡œ ê²€ìƒ‰í•˜ì‹œë©´ ì›í•˜ì‹œëŠ” ìŠ¤íƒ€ì¼ì„ ì°¾ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
        "ë ˆì´ì–´ë§": {
            "reason": "ë ˆì´ì–´ë§ì´ë‚˜ ì¡°í•© ì¶”ì²œì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ê°œë³„ í–¥ìˆ˜ ì¶”ì²œì€ ê°€ëŠ¥í•©ë‹ˆë‹¤! ë ˆì´ì–´ë§ ê´€ë ¨ ì§ˆë¬¸ì€ Scentenceì˜ ë ˆì´ì–´ë§ ê´€ë ¨ ì„œë¹„ìŠ¤ì—ì„œ ì§„í–‰í•´ì£¼ì„¸ìš”!",
        },
        "êµ¬ë§¤ì •ë³´": {
            "reason": "êµ¬ë§¤ì²˜ë‚˜ ë§¤ì¥ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "íŠ¹ì • í–¥ìˆ˜ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
        "ë¬¼ë¦¬ì ": {
            "reason": "ìš©ëŸ‰, í¬ê¸° ë“± ë¬¼ë¦¬ì  ì •ë³´ëŠ” ë³´ìœ í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "í–¥ìˆ˜ì˜ íŠ¹ì„±(ì–´ì½”ë“œ, ë…¸íŠ¸ ë“±)ìœ¼ë¡œ ê²€ìƒ‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
        "ì¶”ì²œ_ì´ìœ ": {
            "reason": "ì¶”ì²œ ê¸°ì¤€ì´ë‚˜ ì´ìœ ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ëŒ€ì‹  ì¶”ì²œëœ í–¥ìˆ˜ì˜ íŠ¹ì„±(ì–´ì½”ë“œ, ë…¸íŠ¸ ë“±)ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?",
        },
        "ë¸Œëœë“œ_ì „ì²´": {
            "reason": "ë¸Œëœë“œ ì „ì²´ ì„¤ëª…ì´ë‚˜ ë¸Œëœë“œì˜ ëª¨ë“  í–¥ìˆ˜ ë‚˜ì—´ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "íŠ¹ì • í–¥ìˆ˜ëª…ì„ ë§ì”€í•´ì£¼ì‹œë©´ í•´ë‹¹ í–¥ìˆ˜ì— ëŒ€í•´ ìƒì„¸íˆ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        },
        "í–¥ìˆ˜_ë¹„êµ": {
            "reason": "ë‘ í–¥ìˆ˜ë¥¼ ë¹„êµí•˜ëŠ” ê¸°ëŠ¥ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ê° í–¥ìˆ˜ì˜ íŠ¹ì„±ì„ ê°œë³„ì ìœ¼ë¡œ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?",
        },
        "ì¸ì‚¬": {
            "reason": "ì•ˆë…•í•˜ì„¸ìš”",
            "alternative": "ì €ëŠ” ë§ì¶¤í˜• í–¥ìˆ˜ì¶”ì²œ AI ì…ë‹ˆë‹¤. í–¥ìˆ˜ì— ê´€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì •í™•í•œ ì •ë³´ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!",
        },
    }

    msg_data = category_messages.get(
        category,
        {
            "reason": "í•´ë‹¹ ìš”ì²­ì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "alternative": "ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        },
    )

    specific_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. {msg_data['reason']}\n\nğŸ’¡ {msg_data['alternative']}"
    if msg_data["reason"] == "ì•ˆë…•í•˜ì„¸ìš”":
        specific_msg = f"{msg_data['reason']}!\n\n{msg_data['alternative']}"

    return {
        "messages": [AIMessage(content=specific_msg)],
        "chat_outcome_status": "UNSUPPORTED_REQUEST",
    }


# ==========================================
# 4. Graph Build
# ==========================================
workflow = StateGraph(AgentState)

workflow.add_node("pre_validator", pre_validator_node)
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
# workflow.add_node("researcher", researcher_node)  # Replaced by parallel_reco
# workflow.add_node("writer", writer_node)  # Replaced by parallel_reco
workflow.add_node("parallel_reco", parallel_reco_node)

# [Wave 2-3] Add status-based handler nodes
workflow.add_node("parallel_reco_ok_writer", parallel_reco_ok_writer)
workflow.add_node("parallel_reco_no_results", parallel_reco_no_results)
workflow.add_node("parallel_reco_error", parallel_reco_error)
workflow.add_node("out_of_scope_handler", out_of_scope_handler)
workflow.add_node("unsupported_request_handler", unsupported_request_handler)
workflow.add_node("info_retrieval_subgraph", call_info_graph_wrapper)

workflow.add_edge(START, "pre_validator")

# Pre-validator routing
workflow.add_conditional_edges(
    "pre_validator",
    lambda x: x.get("validation_result", "supported"),
    {"supported": "supervisor", "unsupported": "unsupported_request_handler"},
)

workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {
        "interviewer": "interviewer",
        "info_retrieval": "info_retrieval_subgraph",
        "writer": "out_of_scope_handler",  # Out-of-scope questions (non-perfume related)
    },
)

workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {"end": END, "researcher": "parallel_reco", "writer": "parallel_reco"},
)

# workflow.add_edge("researcher", "writer")  # Old flow - replaced
# workflow.add_edge("writer", END)  # Old flow - replaced

# [Wave 2-3 - Pattern A] parallel_reco â†’ status ê¸°ë°˜ ì§ì ‘ ë¶„ê¸°
workflow.add_conditional_edges(
    "parallel_reco",
    parallel_reco_result_router,
    {
        "parallel_reco_ok_writer": "parallel_reco_ok_writer",
        "parallel_reco_no_results": "parallel_reco_no_results",
        "parallel_reco_error": "parallel_reco_error",
    },
)

# [Wave 2-3] All status nodes â†’ END
workflow.add_edge("parallel_reco_ok_writer", END)
workflow.add_edge("parallel_reco_no_results", END)
workflow.add_edge("parallel_reco_error", END)
workflow.add_edge("out_of_scope_handler", END)
workflow.add_edge("unsupported_request_handler", END)
workflow.add_edge("info_retrieval_subgraph", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
