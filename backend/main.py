import json
import re
import time
from typing import Generator, List
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
import os
from agent.user_mode import normalize_user_mode
from langchain_core.messages import HumanMessage, AIMessage

# auth ë¼ìš°í„° ë“±ë¡ + chat ê²€ì¦ë°©ì‹ ë³€ê²½ ====ksu====
from fastapi import Depends
from agent.auth import get_identity, require_member_match
from routers import auth


# ëª¨ë“ˆ ì„í¬íŠ¸
from agent.schemas import ChatRequest
from agent.graph import app_graph
from agent.utils import parse_recommended_count, normalize_recommended_count
# [26.02.09 ë³€ê²½ ì´ë ¥ - í”„ë¡ íŠ¸ ìš°í´ë¦­ íˆìŠ¤í† ë¦¬ ì‚­ì œ ëŒ€ì‘]
# [ì´ì „ ì½”ë“œ]
# from agent.database import (
#     save_chat_message,
#     get_chat_history,
#     get_user_chat_list,
#     get_recommended_history,
# )
# [ë³€ê²½ ì´ìœ ]
# - í”„ë¡ íŠ¸(Chat Sidebar)ì—ì„œ ìš°í´ë¦­ìœ¼ë¡œ ëŒ€í™”ë°© ì‚­ì œ ê¸°ëŠ¥ì„ ì¶”ê°€í•¨.
# - ì‚­ì œ APIì—ì„œ threadë¥¼ soft-delete ì²˜ë¦¬í•˜ê¸° ìœ„í•´ DB í•¨ìˆ˜ importê°€ í•„ìš”í•¨.
from agent.database import (
    save_chat_message,
    get_chat_history,
    get_user_chat_list,
    get_recommended_history,
    soft_delete_chat_room,
)
from routers import users, perfumes, archive, auth # <--- ksu ì¶”ê°€

app = FastAPI(title="Perfume Re-Act Chatbot")

uploads_dir = os.path.join(os.getcwd(), "uploads")
os.makedirs(uploads_dir, exist_ok=True)
app.mount("/uploads", StaticFiles(directory=uploads_dir), name="uploads")

app.include_router(users.router)
app.include_router(perfumes.router) # <--- ksu ì¶”ê°€
app.include_router(archive.router) # <--- ksu ì¶”ê°€
app.include_router(auth.router) # <--- ksu ì¶”ê°€ (routers/auth.py)

# CORS origins from environment variable
cors_origins_env = os.getenv("BACKEND_CORS_ORIGINS", "")
if cors_origins_env:
    origins = [origin.strip() for origin in cors_origins_env.split(",") if origin.strip() and origin.strip() != "*"]
else:
    # Default for local development
    origins = ["http://localhost:3000", "http://127.0.0.1:3000"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def resolve_recommended_count_with_flag(
    user_query: str,
    explicit_count: int | None
) -> tuple[int, bool]:
    """
    ì¶”ì²œ ê°œìˆ˜ì™€ ëª…ì‹œì„± ì—¬ë¶€ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        (count, is_explicit)
        - count: ì¶”ì²œ ê°œìˆ˜
        - is_explicit: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í–ˆëŠ”ì§€ ì—¬ë¶€
    """
    # ì¼€ì´ìŠ¤ 1: API íŒŒë¼ë¯¸í„°ë¡œ ëª…ì‹œì  ì „ë‹¬
    if explicit_count is not None:
        normalized = normalize_recommended_count(explicit_count)
        return (normalized, True)

    # ì¼€ì´ìŠ¤ 2: ì¿¼ë¦¬ì—ì„œ ê°œìˆ˜ íŒŒì‹± ì‹œë„
    parsed = parse_recommended_count(user_query)
    if parsed is not None:
        normalized = normalize_recommended_count(parsed)
        return (normalized, True)  # ì¿¼ë¦¬ì— ê°œìˆ˜ê°€ ìˆìœ¼ë©´ ëª…ì‹œì 

    # ì¼€ì´ìŠ¤ 3: ë””í´íŠ¸
    return (3, False)  # ë””í´íŠ¸ëŠ” ë¬µì‹œì 
async def stream_generator(
    user_query: str,
    thread_id: str,
    member_id: int = 0,
    user_mode: str = "BEGINNER",
    recommended_count: int = 3,
) -> Generator[str, None, None]:

    save_chat_message(thread_id, member_id, "user", user_query)
    config = {"configurable": {"thread_id": thread_id}}

    # [â˜… ìˆ˜ì •] íˆìŠ¤í† ë¦¬ ì¤‘ë³µ ë°©ì§€ ë¡œì§
    # checkpointerì— stateê°€ ìˆëŠ”ì§€ í™•ì¸
    try:
        current_state = app_graph.get_state(config)
        has_checkpointed_state = (
            current_state
            and current_state.values
            and current_state.values.get("messages")
        )
    except Exception:
        has_checkpointed_state = False

    # checkpointerê°€ ë¹„ì–´ìˆìœ¼ë©´ (ì„œë²„ ì¬ì‹œì‘ ë“±) DBì—ì„œ ë³µì›
    if not has_checkpointed_state:
        print(f"   ğŸ”„ [History] Checkpointer empty, restoring from DB (thread_id: {thread_id})")
        db_history = get_chat_history(thread_id)
        restored_messages = []

        for msg in db_history:
            if msg["role"] == "user" and msg["text"] == user_query:
                continue
            if msg["role"] == "user":
                restored_messages.append(HumanMessage(content=msg["text"]))
            else:
                restored_messages.append(AIMessage(content=msg["text"]))

        # [â˜…ì¶”ê°€] DBì—ì„œ recommended_history ë³µì›
        db_recommended_history = get_recommended_history(thread_id)

        # ì²« ìš”ì²­: DB ë³µì› ë©”ì‹œì§€ + ìƒˆ ë©”ì‹œì§€
        input_messages = restored_messages + [HumanMessage(content=user_query)]
        print(f"   ğŸ“Š [History] Restored {len(restored_messages)} messages from DB")
    else:
        # checkpointerì— state ìˆìŒ: ìƒˆ ë©”ì‹œì§€ë§Œ ì „ë‹¬
        input_messages = [HumanMessage(content=user_query)]
        existing_count = len(current_state.values.get("messages", []))
        print(f"   âœ… [History] Using checkpointer ({existing_count} existing messages)")

        # [â˜…ì¶”ê°€] Checkpointerì— ì´ë¯¸ recommended_historyê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        db_recommended_history = current_state.values.get("recommended_history", [])

    normalized_mode = normalize_user_mode(user_mode)

    # [â˜…ì¶”ê°€] ì¶”ì²œ ê°œìˆ˜ì™€ ëª…ì‹œì„± ì—¬ë¶€ ê³„ì‚°
    resolved_count, is_explicit = resolve_recommended_count_with_flag(
        user_query, recommended_count if recommended_count != 3 else None
    )

    inputs = {
        "messages": input_messages,
        "member_id": member_id,
        "user_mode": normalized_mode,
        "user_query": user_query,
        "recommended_count": resolved_count,
        "is_count_explicit": is_explicit,  # [â˜…ì¶”ê°€] ëª…ì‹œì„± í”Œë˜ê·¸
        "thread_id": thread_id,  # [â˜…ì¶”ê°€] DB ë°±ì—…ì„ ìœ„í•œ thread_id
        "recommended_history": db_recommended_history,  # [â˜…ì¶”ê°€] DBì—ì„œ ë³µì›í•œ íˆìŠ¤í† ë¦¬
    }

    full_ai_response = ""
    did_stream_parallel_reco = False
    pending_parallel_reco_separator = False

    try:
        async for event in app_graph.astream_events(
            inputs, config=config, version="v2"
        ):
            kind = event["event"]
            metadata = event.get("metadata", {})
            node_name = metadata.get("langgraph_node", "")

            # [1] ë…¸ë“œ ì¢…ë£Œ ì‹œ status ë©”ì‹œì§€ ì²˜ë¦¬ (Supervisor -> Researcher ì „í™˜ ì‹œ ë“±)
            if kind == "on_chain_end":
                output = event["data"].get("output")
                if output and isinstance(output, dict) and "status" in output:
                    status_msg = output["status"]
                    data = json.dumps(
                        {"type": "log", "content": status_msg}, ensure_ascii=False
                    )
                    yield f"data: {data}\n\n"

            # [A] Writer & Info Agents: ì‹¤ì‹œê°„ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
            if kind == "on_chat_model_stream":

                # [â˜…ì¶”ê°€] ë‚´ë¶€ìš© í—¬í¼(ë²ˆì—­ê¸° ë“±)ì˜ ì¶œë ¥ì€ í™”ë©´ì— ë³´ë‚´ì§€ ì•Šê³  ë¬´ì‹œ(Skip)
                tags = event.get("tags", [])
                if "internal_helper" in tags:
                    continue

                target_nodes = [
                    # Recommendation graph
                    "parallel_reco",
                    # Legacy / other graphs
                    "writer",
                    "perfume_describer",
                    "ingredient_specialist",
                    "similarity_curator",
                    # [Wave 2] Info graph status-specific nodes (only streaming ones)
                    "info_writer",
                ]
                # NOTE: LangGraph's node name comes from workflow.add_node("<name>", ...).
                # We include a prefix fallback in case the runtime metadata differs.
                if node_name in target_nodes or node_name.startswith("parallel_reco"):
                    content = event["data"]["chunk"].content
                    if content:
                        if node_name == "parallel_reco" or node_name.startswith(
                            "parallel_reco"
                        ):
                            if pending_parallel_reco_separator and content.lstrip().startswith(
                                "##"
                            ):
                                content = f"\n\n{content.lstrip()}"
                                pending_parallel_reco_separator = False
                            content = content.replace("---##", "---\n\n##").replace(
                                "--- ##", "---\n\n##"
                            )
                        if node_name == "parallel_reco" or node_name.startswith(
                            "parallel_reco"
                        ):
                            did_stream_parallel_reco = True
                            if content.strip().endswith("---"):
                                pending_parallel_reco_separator = True
                        full_ai_response += content
                        data = json.dumps(
                            {"type": "answer", "content": content}, ensure_ascii=False
                        )
                        yield f"data: {data}\n\n"

            # [B] Interviewer & Fixed Message Nodes: ê²°ê³¼ ì „ì†¡ (non-streaming)
            elif kind == "on_chain_end" and node_name in [
                "interviewer",
                # Info graph fixed message nodes
                "fallback_handler",
                "info_no_results",
                "info_error",
                # Main graph fixed message nodes
                "out_of_scope_handler",
                "unsupported_request_handler",
                # Reco graph fixed message nodes
                "parallel_reco_no_results",
                "parallel_reco_error",
            ]:
                output = event["data"].get("output")
                if output and isinstance(output, dict):
                    messages = output.get("messages")
                    if messages and len(messages) > 0:
                        last_msg = messages[-1]
                        if hasattr(last_msg, "content") and last_msg.content:
                            full_ai_response += last_msg.content
                            data = json.dumps(
                                {"type": "answer", "content": last_msg.content},
                                ensure_ascii=False,
                            )
                            yield f"data: {data}\n\n"

            # [B-2] parallel_reco: ì™„ì„±ëœ ê²°ê³¼ ì „ì†¡ (non-streaming)
            elif kind == "on_chain_end" and node_name == "parallel_reco":
                output = event["data"].get("output")
                if output and isinstance(output, dict):
                    messages = output.get("messages")
                    if messages and len(messages) > 0:
                        last_msg = messages[-1]
                        if hasattr(last_msg, "content") and last_msg.content:
                            if did_stream_parallel_reco:
                                # [â˜…ìˆ˜ì •] ìŠ¤íŠ¸ë¦¬ë° í›„ ì¶”ê°€ëœ ë‚´ìš©(ì•ˆë‚´ ë©”ì‹œì§€) ì „ì†¡
                                # ì •ê·œì‹ìœ¼ë¡œ ì•ˆë‚´ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ (ìŠ¬ë¼ì´ì‹± ì˜¤ë¥˜ ë°©ì§€)
                                
                                # ì•ˆë‚´ ë©”ì‹œì§€ íŒ¨í„´
                                notice_patterns = [
                                    r'ğŸ’¡\s*ì•ˆë‚´:.*',  # ê¸°ë³¸ ì•ˆë‚´ íŒ¨í„´
                                ]
                                
                                additional_content = ""
                                for pattern in notice_patterns:
                                    match = re.search(pattern, last_msg.content, re.DOTALL)
                                    if match:
                                        notice_text = match.group(0)
                                        # ì´ë¯¸ ì „ì†¡ëœ ë¶€ë¶„ì¸ì§€ í™•ì¸
                                        if notice_text not in full_ai_response:
                                            additional_content = notice_text
                                            break
                                
                                if additional_content:
                                    full_ai_response += additional_content
                                    data = json.dumps(
                                        {"type": "answer", "content": additional_content},
                                        ensure_ascii=False,
                                    )
                                    yield f"data: {data}\n\n"
                                continue
                            full_ai_response += last_msg.content
                            data = json.dumps(
                                {"type": "answer", "content": last_msg.content},
                                ensure_ascii=False,
                            )
                            yield f"data: {data}\n\n"

            # [C] â˜…Researcher ë‚´ë¶€ ë‹¨ê³„ ì „í™˜ (ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ -> ê²€ìƒ‰ ì‹œì‘)â˜…
            elif kind == "on_chat_model_end" and node_name == "researcher":
                # ë¦¬ì„œì²˜ ë…¸ë“œ ë‚´ì—ì„œ ì „ëµ ìˆ˜ë¦½ LLMì´ ëë‚˜ë©´ ì¦‰ì‹œ ê²€ìƒ‰ ë¬¸êµ¬ë¡œ êµì²´í•©ë‹ˆë‹¤.
                log_msg = "ì „ëµì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ê²€ìƒ‰ì¤‘ ì…ë‹ˆë‹¤..."
                data = json.dumps(
                    {"type": "log", "content": log_msg}, ensure_ascii=False
                )
                yield f"data: {data}\n\n"

            # [D] Tools (ë¡œê·¸): ë°ì´í„° ì¡°íšŒ ì™„ë£Œ
            elif kind == "on_chain_end" and node_name == "tools":
                log_msg = (
                    "âœ… ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ì¤‘ì…ë‹ˆë‹¤..."
                )
                data = json.dumps(
                    {"type": "log", "content": log_msg}, ensure_ascii=False
                )
                yield f"data: {data}\n\n"

        if full_ai_response:
            save_chat_message(thread_id, member_id, "assistant", full_ai_response)

    except GeneratorExit:
        return
    except Exception as e:
        error_msg = json.dumps({"type": "error", "content": str(e)}, ensure_ascii=False)
        yield f"data: {error_msg}\n\n"

# ê¸°ì¡´ ì½”ë“œ ì£¼ì„ì²˜ë¦¬ /chat ë³€ê²½ (request.user_mode ì‹ ë¢°í•˜ì§€ ì•ŠìŒ)
# @app.post("/chat")
# async def chat_stream(request: ChatRequest):
#     recommended_count = request.recommended_count or 3
#     return StreamingResponse(
#         stream_generator(
#             request.user_query,
#             request.thread_id,
#             request.member_id,
#             request.user_mode,
#             recommended_count,
#         ),
#         media_type="text/event-stream",
#         headers={
#             "Cache-Control": "no-cache, no-transform",
#             "Connection": "keep-alive",
#             "X-Accel-Buffering": "no",
#         },
#     )

# ìˆ˜ì • ì½”ë“œ
@app.post("/chat")
async def chat_stream(request: ChatRequest, identity = Depends(get_identity)):
    member_id = identity.user_id or 0
    user_mode = identity.user_mode or "BEGINNER"
    recommended_count = request.recommended_count or 3
    return StreamingResponse(
        stream_generator(
            request.user_query,
            request.thread_id,
            member_id,
            user_mode,
            recommended_count,
        ),
        media_type="text/event-stream",
        # NOTE: ì´ ë³€ê²½ì€ SSE ì‘ë‹µ í—¤ë” ë³µêµ¬ìš©ì´ë©° ì—ì´ì „íŠ¸ ë¡œì§/ì„±ëŠ¥ì—ëŠ” ì˜í–¥ ì—†ìŒ
        # SSE ì‘ë‹µì€ ë°˜ë“œì‹œ dict í—¤ë” í•„ìš” (set ì‚¬ìš© ì‹œ 500 ì—ëŸ¬)
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@app.get("/health")
def health():
    return {"status": "ok"}

# ê¸°ì¡´ ì½”ë“œ ì£¼ì„ì²˜ë¦¬
# @app.get("/chat/rooms/{member_id}")
# async def get_rooms(member_id: int):
#     rooms = get_user_chat_list(member_id)
#     return {"rooms": rooms}

# ============= ksu =============
# ì±„íŒ…ë°© ëª©ë¡ ì¡°íšŒ
@app.get("/chat/rooms/{member_id}")
async def get_rooms(member_id: int, identity = Depends(get_identity)):
    require_member_match(member_id, identity)
    rooms = get_user_chat_list(member_id)
    return {"rooms": rooms}


@app.get("/chat/history/{thread_id}")
async def get_history(thread_id: str):
    messages = get_chat_history(thread_id)
    return {"messages": messages}


# [26.02.09 ë³€ê²½ ì´ë ¥ - ì±„íŒ…ë°© ì‚­ì œ API ì¶”ê°€]
# [ì´ì „ ì½”ë“œ]
# - /chat/rooms/{member_id} (ì¡°íšŒ) ë° /chat/history/{thread_id} (ì¡°íšŒ)ë§Œ ì¡´ì¬
# - ì±„íŒ…ë°© ì‚­ì œ ë¼ìš°íŠ¸ê°€ ì—†ì–´ í”„ë¡ íŠ¸ì—ì„œ íˆìŠ¤í† ë¦¬ ì‚­ì œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ì—ˆìŒ
# [ë³€ê²½ ì´ìœ ]
# - í”„ë¡ íŠ¸ì—ì„œ "ìš°í´ë¦­ ì‚­ì œ" ë™ì‘ì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
# - get_identity + require_member_matchë¡œ ë³¸ì¸ ì†Œìœ  ë°©ë§Œ ì‚­ì œ ê°€ëŠ¥í•˜ë„ë¡ ë³´ì•ˆ ìœ ì§€
@app.delete("/chat/rooms/{member_id}/{thread_id}")
async def delete_room(member_id: int, thread_id: str, identity=Depends(get_identity)):
    require_member_match(member_id, identity)
    deleted = soft_delete_chat_room(member_id, thread_id)
    return {"ok": True, "deleted": deleted}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
